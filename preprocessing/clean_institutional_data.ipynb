{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from preprocessing import parse_raw, pretrained_models, clean_responses\n",
    "\n",
    "from importlib import reload\n",
    "reload(parse_raw)\n",
    "reload(pretrained_models)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# import exhibit, gallery data\n",
    "\n",
    "HOME = os.getenv(\"PROJ_HOME\")\n",
    "exhibits_filepath = os.path.join(HOME, \"data/scraped/exhibits.json\")\n",
    "galleries_filepath = os.path.join(HOME, \"data/scraped/galleries.json\")\n",
    "\n",
    "with open(exhibits_filepath, 'r') as file:\n",
    "    exhibits = json.load(file)\n",
    "\n",
    "with open(galleries_filepath, 'r') as file:\n",
    "    galleries = json.load(file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# transform raw exhibit data into data containing the following fields:\n",
    "#\n",
    "# id, title, aliases (list),\n",
    "# tagline, description, details,\n",
    "# creators, year (both obtained with Cloud Natural Language models),\n",
    "# location (coded), related exhibits, collections,\n",
    "# keywords (some defined in raw data, some obtained with OpenAI models),\n",
    "# summary (obtained with OpenAI models),\n",
    "# fun-facts (obtained with OpenAI models)\n",
    "\n",
    "def init_exhibit(exhibit):\n",
    "    # initialize clean exhibit\n",
    "    this_exhibit = {\n",
    "        \"id\": exhibit[\"id\"],\n",
    "        \"title\": exhibit[\"title\"],\n",
    "        \"aliases\": parse_raw.parse_aliases(exhibit[\"aliases\"]),\n",
    "        \"tagline\": exhibit[\"tagline\"],\n",
    "        \"description\": parse_raw.remove_lang_settings(exhibit[\"description\"]),\n",
    "        \"location\": parse_raw.get_location_code(exhibit[\"location\"]),\n",
    "        \"details\": \" \".join([exhibit[\"whats_going_on\"], exhibit[\"going_further\"], exhibit[\"details\"]]).strip(),\n",
    "        \"related_exhibits\": exhibit[\"related_id\"],\n",
    "        \"collections\": exhibit[\"collection_id\"],\n",
    "        \"keywords\": exhibit[\"phenomena\"] + exhibit[\"keywords\"]\n",
    "    }\n",
    "\n",
    "    # parse byline into creators and year\n",
    "    byline_entities = pretrained_models.get_google_entities(exhibit[\"byline\"])\n",
    "    creators = clean_responses.get_creators(byline_entities)\n",
    "    year = clean_responses.get_year(byline_entities)\n",
    "\n",
    "    this_exhibit.update({\n",
    "        \"creators\": creators,\n",
    "        \"year\": year,\n",
    "    })\n",
    "\n",
    "    # define text field that will be passed to OpenAI models\n",
    "    text_info = exhibit[\"description\"] + \\\n",
    "                exhibit[\"details\"] + \\\n",
    "                exhibit[\"whats_going_on\"] + \\\n",
    "                exhibit[\"going_further\"]\n",
    "\n",
    "    all_text = \" \".join(this_exhibit[\"aliases\"] +\n",
    "                        [\n",
    "                            exhibit[\"title\"],\n",
    "                            exhibit[\"tagline\"],\n",
    "                            text_info\n",
    "                        ] + \\\n",
    "                        exhibit[\"keywords\"] + exhibit[\"phenomena\"]\n",
    "                        )\n",
    "\n",
    "    # extract summary of exhibit, then process it\n",
    "    summary = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"summary\",\n",
    "        domain=\"exhibit\",\n",
    "        text=all_text,\n",
    "        audience=\"an 8th grader\",\n",
    "        temp=0.2\n",
    "    )\n",
    "    summary = clean_responses.remove_frag_start(summary)\n",
    "\n",
    "    # extract new keywords from OpenAI model\n",
    "    new_keywords = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"keywords\",\n",
    "        domain=\"exhibit\",\n",
    "        text=all_text,\n",
    "        temp=0.2\n",
    "    )\n",
    "    new_keywords = clean_responses.find_items(new_keywords)\n",
    "    all_keywords = clean_responses.process_keywords(this_exhibit[\"keywords\"], new_keywords)\n",
    "\n",
    "    # get fun facts about exhibit from OpenAI\n",
    "    fun_facts = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"fun-facts\",\n",
    "        domain=\"exhibit\",\n",
    "        text=all_text,\n",
    "        temp=0.2\n",
    "    )\n",
    "    fun_facts = clean_responses.find_items(fun_facts, short=False)\n",
    "\n",
    "    this_exhibit.update({\n",
    "        \"summary\": summary,\n",
    "        \"keywords\": all_keywords,\n",
    "        \"fun-facts\": fun_facts,\n",
    "    })\n",
    "\n",
    "    return this_exhibit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# transform raw gallery data into data containing the following fields:\n",
    "# id, title,\n",
    "# tagline, description,\n",
    "# keywords (obtained with OpenAI models),\n",
    "# summary (obtained with OpenAI models),\n",
    "# fun-facts (obtained with OpenAI models)\n",
    "\n",
    "def init_gallery(gallery):\n",
    "    # initialize clean gallery\n",
    "    this_gallery = {\n",
    "        \"id\": gallery[\"id\"],\n",
    "        \"title\": gallery[\"title\"],\n",
    "        \"tagline\": gallery[\"tagline\"],\n",
    "        \"description\": gallery[\"description\"],\n",
    "        \"curator_statement\": gallery[\"curator_statement\"]\n",
    "    }\n",
    "\n",
    "    # define text field that will be passed to OpenAI models\n",
    "    all_text = \" \".join([gallery[\"title\"],\n",
    "                         gallery[\"tagline\"],\n",
    "                         gallery[\"description\"],\n",
    "                         \"Curator Statement:\",\n",
    "                         gallery[\"curator_statement\"],\n",
    "                        ])\n",
    "\n",
    "    # extract summary of gallery, then process it\n",
    "    summary = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"summary\",\n",
    "        domain=\"gallery\",\n",
    "        text=all_text,\n",
    "        audience=\"an 8th grader\",\n",
    "    )\n",
    "    summary = clean_responses.remove_frag_start(summary)\n",
    "\n",
    "    # extract new keywords from OpenAI model, then process\n",
    "    keywords = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"keywords\",\n",
    "        domain=\"gallery\",\n",
    "        text=all_text,\n",
    "        temp=0.2\n",
    "    )\n",
    "    keywords = clean_responses.find_items(keywords)\n",
    "\n",
    "    # get fun facts about gallery from OpenAI\n",
    "    fun_facts = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"fun-facts\",\n",
    "        domain=\"gallery\",\n",
    "        text=all_text,\n",
    "        temp=0.5\n",
    "    )\n",
    "    fun_facts = clean_responses.find_items(fun_facts, short=False)\n",
    "\n",
    "    this_gallery.update({\n",
    "        \"summary\": summary,\n",
    "        \"keywords\": keywords,\n",
    "        \"fun-facts\": fun_facts,\n",
    "    })\n",
    "\n",
    "    return this_gallery"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# clean exhibit data\n",
    "\n",
    "init_exhibits = []\n",
    "for exhibit in exhibits:\n",
    "    init_exhibits.append(init_exhibit(exhibit))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# clean gallery data\n",
    "\n",
    "init_galleries = []\n",
    "for gallery in galleries:\n",
    "    init_galleries.append(init_gallery(gallery))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "# save a copy of exhibit and gallery data to disk\n",
    "\n",
    "init_exhibits_cache_path = os.path.join(HOME, \"data/institutional/exhibits.json\")\n",
    "with open(init_exhibits_cache_path, \"w\") as outfile:\n",
    "    json.dump(init_exhibits, outfile, indent=2)\n",
    "\n",
    "init_galleries_cache_path = os.path.join(HOME, \"data/institutional/galleries.json\")\n",
    "with open(init_galleries_cache_path, \"w\") as outfile:\n",
    "    json.dump(init_galleries, outfile, indent=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
