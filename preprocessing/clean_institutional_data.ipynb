{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "\n",
    "from preprocessing import scripts, pretrained_models\n",
    "from importlib import reload\n",
    "reload(scripts)\n",
    "reload(pretrained_models)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# import exhibit, gallery data\n",
    "\n",
    "home = os.getenv(\"PROJ_HOME\")\n",
    "exhibits_filepath = os.path.join(home, \"data/raw/exhibits.json\")\n",
    "galleries_filepath = os.path.join(home, \"data/raw/galleries.json\")\n",
    "\n",
    "with open(exhibits_filepath, 'r') as file:\n",
    "    exhibits = json.load(file)\n",
    "\n",
    "with open(galleries_filepath, 'r') as file:\n",
    "    galleries = json.load(file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "# transform raw exhibit data into data containing the following fields:\n",
    "# id, title, aliases (list),\n",
    "# tagline, description, details,\n",
    "# creators, year (both obtained with Cloud Natural Language models),\n",
    "# location (coded), related exhibits, collections,\n",
    "# keywords (some defined in raw data, some obtained with OpenAI models),\n",
    "# short-summary, long-summary (both obtained with OpenAI models),\n",
    "# fun-facts (obtained with OpenAI models)\n",
    "\n",
    "def init_exhibit(exhibit):\n",
    "    # initialize clean exhibit\n",
    "    this_exhibit = {\n",
    "        \"id\": exhibit[\"id\"],\n",
    "        \"title\": exhibit[\"title\"],\n",
    "        \"aliases\": scripts.parse_aliases(exhibit[\"aliases\"]),\n",
    "        \"tagline\": exhibit[\"tagline\"],\n",
    "        \"description\": scripts.remove_lang_settings(exhibit[\"description\"]),\n",
    "        \"location\": scripts.get_location_code(exhibit[\"location\"]),\n",
    "        \"details\": \" \".join([exhibit[\"whats_going_on\"], exhibit[\"going_further\"], exhibit[\"details\"]]).strip(),\n",
    "        \"related_exhibits\": exhibit[\"related_id\"],\n",
    "        \"collections\": exhibit[\"collection_id\"],\n",
    "        \"keywords\": exhibit[\"phenomena\"] + exhibit[\"keywords\"]\n",
    "    }\n",
    "\n",
    "    # parse byline into creators and year\n",
    "    byline_entities = pretrained_models.get_google_entities(exhibit[\"byline\"])\n",
    "    creators = scripts.get_creators(byline_entities)\n",
    "    year = scripts.get_year(byline_entities)\n",
    "\n",
    "    this_exhibit.update({\n",
    "        \"creators\": creators,\n",
    "        \"year\": year,\n",
    "    })\n",
    "\n",
    "    # define text field that will be passed to OpenAI models\n",
    "    text_info = exhibit[\"description\"] + \\\n",
    "                exhibit[\"details\"] + \\\n",
    "                exhibit[\"whats_going_on\"] + \\\n",
    "                exhibit[\"going_further\"]\n",
    "\n",
    "    all_text = \" \".join(this_exhibit[\"aliases\"] +\n",
    "                        [\n",
    "                            exhibit[\"title\"],\n",
    "                            exhibit[\"tagline\"],\n",
    "                            text_info\n",
    "                        ] + \\\n",
    "                        exhibit[\"keywords\"] + exhibit[\"phenomena\"]\n",
    "                        )\n",
    "\n",
    "    # extract short summary of exhibit, then process it\n",
    "    short_summary = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"short-summary\",\n",
    "        domain=\"exhibit\",\n",
    "        text=all_text,\n",
    "        audience=\"an 8th grader\",\n",
    "        temp=0.2\n",
    "    )\n",
    "    short_summary = scripts.remove_frag_start(short_summary)\n",
    "\n",
    "    # extract medium summary of exhibit, if enough data about the exibit is available; then process\n",
    "    enough_data = True if len(text_info) > 200 else False\n",
    "    if enough_data:\n",
    "        medium_summary = pretrained_models.get_openai_completion(\n",
    "            engine=\"text-davinci-001\",\n",
    "            prompt_type=\"medium-summary\",\n",
    "            domain=\"exhibit\",\n",
    "            text=all_text,\n",
    "            audience=\"an 8th grader\",\n",
    "            temp=0.2\n",
    "        )\n",
    "    else:\n",
    "        medium_summary = \"\"\n",
    "    medium_summary = scripts.remove_frag_start(medium_summary)\n",
    "\n",
    "    # extract new keywords from OpenAI model\n",
    "    new_keywords = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"keywords\",\n",
    "        domain=\"exhibit\",\n",
    "        text=all_text,\n",
    "        temp=0.2\n",
    "    )\n",
    "    new_keywords = scripts.find_items(new_keywords)\n",
    "    all_keywords = scripts.process_keywords(this_exhibit[\"keywords\"], new_keywords)\n",
    "\n",
    "    # get fun facts about exhibit from OpenAI\n",
    "    fun_facts = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"fun-facts\",\n",
    "        domain=\"exhibit\",\n",
    "        text=all_text,\n",
    "        temp=0.2\n",
    "    )\n",
    "    fun_facts = scripts.find_items(fun_facts, short=False)\n",
    "\n",
    "    this_exhibit.update({\n",
    "        \"short-summary\": short_summary,\n",
    "        \"medium-summary\": medium_summary,\n",
    "        \"keywords\": all_keywords,\n",
    "        \"fun-facts\": fun_facts,\n",
    "    })\n",
    "\n",
    "    return this_exhibit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "# transform raw gallery data into data containing the following fields:\n",
    "# id, title,\n",
    "# tagline, description,\n",
    "# keywords (obtained with OpenAI models),\n",
    "# short-summary, medium-summary (both obtained with OpenAI models),\n",
    "# fun-facts (obtained with OpenAI models)\n",
    "\n",
    "def init_gallery(gallery):\n",
    "    # initialize clean gallery\n",
    "    this_gallery = {\n",
    "        \"id\": gallery[\"id\"],\n",
    "        \"title\": gallery[\"title\"],\n",
    "        \"tagline\": gallery[\"tagline\"],\n",
    "        \"description\": gallery[\"description\"],\n",
    "        \"curator_statement\": gallery[\"curator_statement\"]\n",
    "    }\n",
    "\n",
    "    # define text field that will be passed to OpenAI models\n",
    "    all_text = \" \".join([gallery[\"title\"],\n",
    "                         gallery[\"tagline\"],\n",
    "                         gallery[\"description\"],\n",
    "                         \"Curator Statement:\",\n",
    "                         gallery[\"curator_statement\"],\n",
    "                        ])\n",
    "\n",
    "    # extract short summary of gallery, then process it\n",
    "    short_summary = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"short-summary\",\n",
    "        domain=\"gallery\",\n",
    "        text=all_text,\n",
    "        audience=\"an 8th grader\",\n",
    "    )\n",
    "    short_summary = scripts.remove_frag_start(short_summary)\n",
    "\n",
    "    # extract medium summary of gallery, then process it\n",
    "    medium_summary = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"medium-summary\",\n",
    "        domain=\"gallery\",\n",
    "        text=all_text,\n",
    "        audience=\"an 8th grader\",\n",
    "    )\n",
    "    medium_summary = scripts.remove_frag_start(medium_summary)\n",
    "\n",
    "    # extract new keywords from OpenAI model, then process\n",
    "    keywords = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"keywords\",\n",
    "        domain=\"gallery\",\n",
    "        text=all_text,\n",
    "        temp=0.2\n",
    "    )\n",
    "    keywords = scripts.find_items(keywords)\n",
    "\n",
    "    # get fun facts about gallery from OpenAI\n",
    "    fun_facts = pretrained_models.get_openai_completion(\n",
    "        engine=\"text-davinci-001\",\n",
    "        prompt_type=\"fun-facts\",\n",
    "        domain=\"gallery\",\n",
    "        text=all_text,\n",
    "        temp=0.5\n",
    "    )\n",
    "    fun_facts = scripts.find_items(fun_facts, short=False)\n",
    "\n",
    "    this_gallery.update({\n",
    "        \"short-summary\": short_summary,\n",
    "        \"medium-summary\": medium_summary,\n",
    "        \"keywords\": keywords,\n",
    "        \"fun-facts\": fun_facts,\n",
    "    })\n",
    "\n",
    "    return this_gallery"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "# clean exhibit data\n",
    "\n",
    "init_exhibits = []\n",
    "for exhibit in exhibits:\n",
    "    init_exhibits.append(init_exhibit(exhibit))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean gallery data\n",
    "\n",
    "init_galleries = []\n",
    "for gallery in galleries:\n",
    "    init_galleries.append(init_gallery(gallery))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save a copy of exhibit and gallery data to disk\n",
    "\n",
    "init_exhibits_cache_path = os.path.join(home, \"data/cache/init_exhibits.json\")\n",
    "with open(init_exhibits_cache_path, \"w\") as outfile:\n",
    "    json.dump(init_exhibits, outfile, indent=2)\n",
    "\n",
    "init_galleries_cache_path = os.path.join(home, \"data/cache/init_galleries.json\")\n",
    "with open(init_galleries_cache_path, \"w\") as outfile:\n",
    "    json.dump(init_galleries, outfile, indent=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The following fixes are still needed:\n",
    "# - check for duplicate keywords (does this matter?)\n",
    "# - unidecode?\n",
    "# - add sentence-parsing to fun-facts processing"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
