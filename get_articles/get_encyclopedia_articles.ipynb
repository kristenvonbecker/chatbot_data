{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from get_articles import get_data\n",
    "from get_articles import process_articles\n",
    "\n",
    "from importlib import reload\n",
    "reload(get_data)\n",
    "reload(process_articles)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "home = os.getenv(\"PROJ_HOME\")\n",
    "\n",
    "institutional_data_path = os.path.join(home, \"data/institutional\")\n",
    "subject_matter_path = os.path.join(home, \"data/subject_matter\")\n",
    "article_ids_path = os.path.join(home, \"data/cache/article_ids.json\")\n",
    "xml_dir = os.path.join(home, \"data/subject_matter/xml\")\n",
    "text_dir = os.path.join(home, \"data/cache/subject_matter_text\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [],
   "source": [
    "# get encyclopedia metadata\n",
    "\n",
    "def get_metadata(sources=[\"advanced\"], path=None):\n",
    "    articles = {}\n",
    "\n",
    "    for source in sources:\n",
    "        articles_in_source = get_data.get_encyclopedia_metadata(source, path)\n",
    "        articles[source] = articles_in_source\n",
    "\n",
    "    # remove articles with empty title fields\n",
    "    for source in sources:\n",
    "        for article in articles[source]:\n",
    "            if article['title'] == '':\n",
    "                articles[source].remove(article)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for source in sources:\n",
    "        filepath = os.path.join(path, source + '.json')\n",
    "\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "\n",
    "        with open(filepath, \"w\") as outfile:\n",
    "            outfile.write(json.dumps(articles[source], indent=2))\n",
    "\n",
    "    return articles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "# advanced_metadata = get_metadata(path=subject_matter_path)\n",
    "\n",
    "filepath = os.path.join(subject_matter_path, \"advanced.json\")\n",
    "with open(filepath, \"r\") as infile:\n",
    "    advanced_metadata = json.load(infile)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "# load matched article_ids\n",
    "\n",
    "with open(article_ids_path, \"r\") as infile:\n",
    "    matched_article_ids = json.load(infile)\n",
    "\n",
    "# subset metadata by article_id in matched_article_id\n",
    "\n",
    "matched_metadata = []\n",
    "for item in advanced_metadata:\n",
    "    if item[\"articleId\"] in matched_article_ids:\n",
    "        matched_item = {\n",
    "            \"article_id\": item[\"articleId\"],\n",
    "            \"title\": item[\"title\"],\n",
    "            \"last_updated\": item[\"lastUpdated\"]\n",
    "        }\n",
    "        matched_metadata.append(matched_item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "# get the articles (xml content) from the API\n",
    "\n",
    "for article_id in matched_article_ids:\n",
    "    xml = get_data.get_article_xml(article_id=article_id, dir_path=xml_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "# get the text of each article as a list of strings, each string representing a paragraph\n",
    "# encoding?\n",
    "\n",
    "for filename in os.listdir(xml_dir):\n",
    "    article_id = int(filename.rstrip(\".xml\"))\n",
    "    filepath = os.path.join(xml_dir, filename)\n",
    "    with open(filepath, \"r\") as infile:\n",
    "        data = infile.read()\n",
    "    xml_data = BeautifulSoup(data, \"xml\")\n",
    "    get_data.get_article_paragraphs(xml_data, article_id=article_id, dir_path=text_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "# combine matched_metadata and text data on article_id -> matched_data\n",
    "\n",
    "matched_data = []\n",
    "for item in matched_metadata:\n",
    "    this_match = item.copy()\n",
    "    article_id = item[\"article_id\"]\n",
    "    filename = str(article_id) + \".json\"\n",
    "    filepath = os.path.join(text_dir, filename)\n",
    "    with open(filepath, \"r\") as infile:\n",
    "        paragraphs = json.load(infile)\n",
    "    this_match.update({\n",
    "        \"paragraphs\": paragraphs\n",
    "    })\n",
    "    matched_data.append(this_match)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "# clean the first paragraph of each matched article\n",
    "\n",
    "for item in matched_data:\n",
    "    title = item[\"title\"]\n",
    "    par_0 = item[\"paragraphs\"][0]\n",
    "    par_0_new, title_new, aliases, grouping, is_person = process_articles.clean_par_0(paragraph=par_0, title=title)\n",
    "    paragraphs = [par_0_new]\n",
    "    item[\"title\"] = title_new\n",
    "    for par in item[\"paragraphs\"][1:]:\n",
    "        paragraphs.append(process_articles.clean_par_n(par))\n",
    "    item[\"paragraphs\"] = paragraphs\n",
    "    item.update({\n",
    "        \"aliases\": aliases,\n",
    "        \"field\": grouping,\n",
    "        \"is_person\": is_person\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "outfile_path = os.path.join(subject_matter_path, \"article_data.json\")\n",
    "with open(outfile_path, \"w\") as outfile:\n",
    "    json.dump(matched_data, outfile, indent=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
