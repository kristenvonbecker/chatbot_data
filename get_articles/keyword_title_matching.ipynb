{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vonbecker/anaconda3/envs/explorer_env/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import process_keywords\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "\n",
    "from importlib import reload\n",
    "reload(process_keywords)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import keyword data\n",
    "\n",
    "HOME = os.getenv(\"PROJ_HOME\")\n",
    "exhibits_filepath = os.path.join(HOME, \"data/institutional/exhibits.json\")\n",
    "galleries_filepath = os.path.join(HOME, \"data/institutional/galleries.json\")\n",
    "\n",
    "with open(exhibits_filepath, 'r') as file:\n",
    "    exhibits = json.load(file)\n",
    "\n",
    "keyword_data = []\n",
    "for exhibit in exhibits:\n",
    "    this_data = {\n",
    "        \"id\": exhibit[\"id\"],\n",
    "        \"type\": \"exhibit\",\n",
    "        \"keywords\": exhibit[\"keywords\"]\n",
    "    }\n",
    "    keyword_data.append(this_data)\n",
    "\n",
    "with open(galleries_filepath, 'r') as file:\n",
    "    galleries = json.load(file)\n",
    "\n",
    "for gallery in galleries:\n",
    "    this_data = {\n",
    "        \"id\": [\"id\"],\n",
    "        \"type\": \"gallery\",\n",
    "        \"keywords\": gallery[\"keywords\"]\n",
    "    }\n",
    "    keyword_data.append(this_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# import article metadata\n",
    "\n",
    "metadata_filepath = os.path.join(home, \"data/subject_matter/metadata/advanced.json\")\n",
    "\n",
    "with open(metadata_filepath, \"r\") as infile:\n",
    "    metadata = json.load(infile)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1657 keywords\n",
      "There are 938 unique keywords\n"
     ]
    }
   ],
   "source": [
    "# create lists of all keywords, and all unique keywords\n",
    "\n",
    "all_keywords = []\n",
    "for item in keyword_data:\n",
    "    all_keywords += item[\"keywords\"]\n",
    "\n",
    "unique_keywords = list(set(all_keywords))\n",
    "\n",
    "print(\"There are {} keywords\".format(len(all_keywords)))\n",
    "print(\"There are {} unique keywords\".format(len(unique_keywords)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# tokenize and vectorize keywords\n",
    "\n",
    "keyword_tokens = []\n",
    "keyword_vecs = []\n",
    "\n",
    "for keyword in unique_keywords:\n",
    "    tokens = process_keywords.clean_tokenize(keyword, stopwords=stopwords.words(\"english\"))\n",
    "    keyword_tokens.append(tokens)\n",
    "    vectors = process_keywords.vectorize(tokens)\n",
    "    keyword_vecs.append(vectors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# get GloVe embeddings for titles\n",
    "\n",
    "title_embeddings = {}\n",
    "\n",
    "for item in metadata:\n",
    "    title = item[\"title\"]\n",
    "    tokens = process_keywords.clean_tokenize(title, stopwords=stopwords.words(\"english\"))\n",
    "    vectors = np.array(process_keywords.vectorize(tokens))\n",
    "    avg_vector = np.mean(vectors, axis=0)\n",
    "    if not all([x == 0 for x in avg_vector]):\n",
    "        title_embeddings[title] = avg_vector\n",
    "\n",
    "unique_titles = [item[\"title\"] for item in metadata]\n",
    "unique_titles = list(set(unique_titles))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 70872 non-trivial title embeddings\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} non-trivial title embeddings\".format(len(title_embeddings)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# find \"nearest\" titles for each unique keyword\n",
    "\n",
    "title_matches = []\n",
    "\n",
    "for keyword in unique_keywords:\n",
    "    new_matches = process_keywords.find_nearest_titles(\n",
    "        keyword,\n",
    "        titles=unique_titles,\n",
    "        title_embeddings=title_embeddings,\n",
    "        method=\"embedding\",\n",
    "        num=1\n",
    "    )\n",
    "    new_matches += process_keywords.find_nearest_titles(\n",
    "        keyword,\n",
    "        titles=unique_titles,\n",
    "        title_embeddings=title_embeddings,\n",
    "        method=\"fuzz-matching\",\n",
    "        num=1\n",
    "    )\n",
    "    new_matches = list(set(new_matches))\n",
    "    title_matches += new_matches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1024 titles matched\n",
      "There are 919 unique titles matched\n"
     ]
    }
   ],
   "source": [
    "unique_title_matches = list(set(title_matches))\n",
    "\n",
    "print(\"There are {} titles matched\".format(len(title_matches)))\n",
    "print(\"There are {} unique titles matched\".format(len(unique_title_matches)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# get article_ids for title matches\n",
    "\n",
    "article_id_matches = []\n",
    "\n",
    "for title in unique_title_matches:\n",
    "    for item in metadata:\n",
    "        if item[\"title\"] == title:\n",
    "            article_id_matches.append(item[\"articleId\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 969 matched articles\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} matched articles\".format(len(article_id_matches)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# cache the list of article_ids\n",
    "\n",
    "article_ids_filepath = os.path.join(HOME, \"data/cache/article_ids.json\")\n",
    "\n",
    "if os.path.exists(article_ids_filepath):\n",
    "    os.remove(article_ids_filepath)\n",
    "\n",
    "with open(article_ids_filepath, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(article_id_matches))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
